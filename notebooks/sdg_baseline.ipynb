{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_theme()\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib.vae_models import VAE, CVAE\n",
    "import lib.datasets as datasets\n",
    "import lib.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'goi4_dp_small_2yrs_inpost'\n",
    "RANDOM_SEED = 2112\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1011780 consumption profiles from 730 dates and 1386 users\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'../data/{DATASET_NAME}/dataset.csv')\n",
    "data, dates, users = df.iloc[:,:-2].values, df.date.values, df.user.values\n",
    "date_ids, user_ids = df.date.unique(), df.user.unique()\n",
    "num_days, num_users = len(date_ids), len(user_ids)\n",
    "print(f'Loaded {len(data)} consumption profiles from {num_days} dates and {num_users} users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = np.load(f'../data/{DATASET_NAME}/encode_dict.npy', allow_pickle=True).item()[\"date_dict\"]\n",
    "date_dict_inv = {v: k for k, v in date_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'../data/{DATASET_NAME}/raw_dates.npy'):\n",
    "    raw_dates = np.array([datetime.datetime.strptime(date_dict_inv[d], '%Y-%m-%d') for d in dates])\n",
    "    np.save(f'../data/{DATASET_NAME}/raw_dates.npy', raw_dates)\n",
    "else:\n",
    "    raw_dates = np.load(f'../data/{DATASET_NAME}/raw_dates.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = np.array([d.month for d in raw_dates])\n",
    "weekdays = np.array([d.weekday() for d in raw_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_MONTHS = True\n",
    "ADD_WEEKDAYS = True\n",
    "\n",
    "condition_kwargs[\"tags\"], condition_kwargs[\"types\"], condition_kwargs[\"supports\"]  = [], [], []\n",
    "if ADD_MONTHS: \n",
    "    condition_kwargs[\"tags\"].append(\"months\")\n",
    "    condition_kwargs[\"types\"].append(\"circ\")\n",
    "    condition_kwargs[\"supports\"].append(np.unique(months).tolist())\n",
    "if ADD_WEEKDAYS: \n",
    "    condition_kwargs[\"tags\"].append(\"weekdays\")\n",
    "    condition_kwargs[\"types\"].append(\"circ\")\n",
    "    condition_kwargs[\"supports\"].append(np.unique(weekdays).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioner = datasets.Conditioner(**condition_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_set = {\"months\": months, \"weekdays\": weekdays}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 1 #in hours\n",
    "\n",
    "if RESOLUTION == 12:\n",
    "    X = np.reshape(data, (-1, 24))\n",
    "    X = np.reshape(np.concatenate([X[:,6:], X[:,:6]], axis=-1), (num_users, num_days, int(24/RESOLUTION), int(RESOLUTION))).sum(axis=-1)    #circle shift the last dimension of X\n",
    "else:\n",
    "    X = np.reshape(data, (num_users, num_days, int(24/RESOLUTION), int(RESOLUTION))).sum(axis=-1)\n",
    "\n",
    "# condition_set = np.reshape(condition_set, (num_users, num_days, -1))\n",
    "condition_set = {k: np.reshape(v, (num_users, num_days, -1)) for k, v in condition_set.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 4 users with all-zero consumption profiles\n",
      "Removing 18 users with any-negative consumption profiles\n"
     ]
    }
   ],
   "source": [
    "nonzero_user_mask = np.sum(np.all(X == 0, axis=2), axis=1) < num_days\n",
    "print(f'Removing {(~nonzero_user_mask).sum()} users with all-zero consumption profiles')\n",
    "positive_user_mask = np.sum(np.any(X < 0, axis=2), axis=1) == 0\n",
    "print(f'Removing {(~positive_user_mask).sum()} users with any-negative consumption profiles')\n",
    "user_mask = nonzero_user_mask & positive_user_mask\n",
    "X = X[user_mask]\n",
    "condition_set = {k: v[user_mask] for k, v in condition_set.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ampute the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of missing days: 57.19\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "n, a, b = num_days, 0.85, 10.0\n",
    "# n, a, b = num_days, 1.0, 1.0\n",
    "missing_days = np.random.binomial(n, p=np.random.beta(a, b, size=X.shape[0]), size=X.shape[0])\n",
    "print(f\"Mean of missing days: {n*a/(a+b):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing = X.copy().astype(float)\n",
    "condition_missing = {k: v.copy().astype(float) for k, v in condition_set.items()}\n",
    "\n",
    "for user in range(X.shape[0]): \n",
    "    X_missing[user, :missing_days[user]] = np.nan\n",
    "    for k in condition_missing.keys():\n",
    "        condition_missing[k][user, :missing_days[user]] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsample the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (subsampled/filtered) users....1364\n",
      "Number of (subsampled) days...............730\n",
      "Number of (aggregated) features............24\n"
     ]
    }
   ],
   "source": [
    "USER_SUBSAMPLE_RATE, DAY_SUBSAMPLE_RATE = 1, 1\n",
    "X, X_missing = X[::USER_SUBSAMPLE_RATE, ::DAY_SUBSAMPLE_RATE, :], X_missing[::USER_SUBSAMPLE_RATE, ::DAY_SUBSAMPLE_RATE, :]\n",
    "condition_set = {k: v[::USER_SUBSAMPLE_RATE, ::DAY_SUBSAMPLE_RATE, :] for k, v in condition_set.items()}\n",
    "condition_missing = {k: v[::USER_SUBSAMPLE_RATE, ::DAY_SUBSAMPLE_RATE, :] for k, v in condition_missing.items()}\n",
    "num_users, num_days, num_features = X.shape\n",
    "X_gt_list = [X[user, :missing_days[user]]*1 for user in range(num_users)]\n",
    "X_gt_condition_list = {k: [v[user, :missing_days[user]]*1 for user in range(num_users)] for k, v in condition_set.items()}\n",
    "\n",
    "print(\"{:.<40}{:.>5}\".format(\"Number of (subsampled/filtered) users\", num_users))\n",
    "print(\"{:.<40}{:.>5}\".format(\"Number of (subsampled) days\", num_days))\n",
    "print(\"{:.<40}{:.>5}\".format(\"Number of (aggregated) features\", num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_idx_mat  = np.isnan(X_missing).any(2)\n",
    "missing_num_labels = {\"user\": missing_idx_mat.sum(1), \"day\": missing_idx_mat.sum(0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing = X_missing.reshape(-1, num_features)\n",
    "conditions_missing = {k: v.reshape(-1, v.shape[-1]) for k, v in condition_missing.items()}\n",
    "missing_idx = np.isnan(X_missing.sum(1))\n",
    "X_missing = X_missing[~missing_idx]\n",
    "conditions_missing = {k: v[~missing_idx] for k, v in conditions_missing.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Training Data with Missing Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_mean, nonzero_std = utils.zero_preserved_log_stats(X_missing)\n",
    "X_missing = utils.zero_preserved_log_normalize(X_missing, nonzero_mean, nonzero_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the X_missing and conditions_missing into training and validation sets\n",
    "VAL_RATIO = 0.01\n",
    "random_idx = np.random.permutation(len(X_missing))\n",
    "val_idx = random_idx[:int(len(X_missing)*VAL_RATIO)]\n",
    "train_idx = random_idx[int(len(X_missing)*VAL_RATIO):]\n",
    "\n",
    "X_train, X_val = X_missing[train_idx], X_missing[val_idx]\n",
    "conditions_train = {k: v[train_idx] for k, v in conditions_missing.items()}\n",
    "conditions_val = {k: v[val_idx] for k, v in conditions_missing.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Points: 907993\n",
      "Number of Validation Points: 9171\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.ConditionedDataset(inputs=X_train, conditions=conditions_train, conditioner=conditioner)\n",
    "valset = datasets.ConditionedDataset(inputs=X_val, conditions=conditions_val, conditioner=conditioner)\n",
    "print(f\"Number of Training Points: {len(trainset)}\")\n",
    "print(f\"Number of Validation Points: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "                \"latent_dim\": 10,\n",
    "                \"condencoding_dim\": None,\n",
    "                \"posterior_dist\": 'normal',\n",
    "                \"likelihood_dist\": 'mixed',\n",
    "                \"learn_decoder_sigma\": True,\n",
    "                \"num_neurons\": 100,\n",
    "                \"num_hidden_layers\": 3,\n",
    "                \"dropout\": True,\n",
    "                \"dropout_rate\": 0.25,\n",
    "                \"batch_normalization\": True,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(input_dim=num_features, conditioner=conditioner, **model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = {\n",
    "                \"lr\": 1e-4,\n",
    "                \"beta\": 1.0,\n",
    "                \"num_mc_samples\": 1,\n",
    "                \"epochs\": 1000,\n",
    "                \"verbose_freq\": 100,\n",
    "                \"tensorboard\": True,\n",
    "                \"batch_size\": 1024,\n",
    "                \"validation_freq\": 100\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_kwargs[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=train_kwargs[\"batch_size\"], drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbolat/miniconda3/envs/torchenv/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Epoch:   0%|          | 1/1000 [00:00<14:36,  1.14it/s]/home/kbolat/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "                                                       \n",
      "Epoch:   0%|          | 1/1000 [00:13<14:36,  1.14it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- ELBO=-6.59e+01 / RLL=-6.43e+01 / KL=1.64e+00\n",
      "Iteration: 100 -- ELBO=-6.88e+01 / RLL=-6.69e+01 / KL=1.89e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "Epoch:   0%|          | 1/1000 [00:24<14:36,  1.14it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- ELBO=-4.37e+01 / RLL=-4.21e+01 / KL=1.68e+00\n",
      "Iteration: 200 -- ELBO=-4.63e+01 / RLL=-4.44e+01 / KL=1.93e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "Epoch:   0%|          | 1/1000 [00:35<14:36,  1.14it/s]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- ELBO=-3.76e+01 / RLL=-3.59e+01 / KL=1.67e+00\n",
      "Iteration: 300 -- ELBO=-3.75e+01 / RLL=-3.56e+01 / KL=1.86e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/behavio-temporal-vae/lib/vae_models.py:152\u001b[0m, in \u001b[0;36mVAE.fit\u001b[0;34m(self, trainloader, valloader, lr, beta, num_mc_samples, epochs, verbose_freq, tensorboard, tqdm_func, validation_freq, **_)\u001b[0m\n\u001b[1;32m    150\u001b[0m itx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    151\u001b[0m pbar_itx\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m## region Validation\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m itx\u001b[38;5;241m%\u001b[39mvalidation_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m itx\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m valloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Python/behavio-temporal-vae/lib/vae_models.py:228\u001b[0m, in \u001b[0;36mCVAE.train_core\u001b[0;34m(self, inputs, optim, **_)\u001b[0m\n\u001b[1;32m    226\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(inputs, x_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m], z_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m], beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m], prior_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_params)\n\u001b[1;32m    227\u001b[0m loss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 228\u001b[0m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model.fit(trainloader=trainloader, valloader=valloader, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CVAE(\n",
       "  (encoder): GaussianNN(\n",
       "    (parameterizer): ParameterizerNN(\n",
       "      (block_dict): ModuleDict(\n",
       "        (input): NNBlock(\n",
       "          (input_layer): Sequential(\n",
       "            (0): Linear(in_features=28, out_features=50, bias=True)\n",
       "            (1): Softplus(beta=1, threshold=20)\n",
       "          )\n",
       "          (middle_layers): ModuleList(\n",
       "            (0-1): 2 x Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "          )\n",
       "          (output_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (1): Softplus(beta=1, threshold=20)\n",
       "            (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mu): NNBlock(\n",
       "          (input_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (1): Softplus(beta=1, threshold=20)\n",
       "          )\n",
       "          (middle_layers): ModuleList()\n",
       "          (output_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (sigma): NNBlock(\n",
       "          (input_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (1): Softplus(beta=1, threshold=20)\n",
       "          )\n",
       "          (middle_layers): ModuleList()\n",
       "          (output_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): MixedNN(\n",
       "    (discerete_dist): BernoulliNN(\n",
       "      (parameterizer): ParameterizerNN(\n",
       "        (block_dict): ModuleDict(\n",
       "          (input): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=14, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList(\n",
       "              (0-1): 2 x Sequential(\n",
       "                (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "                (1): Softplus(beta=1, threshold=20)\n",
       "              )\n",
       "            )\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "          )\n",
       "          (pi): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList()\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=24, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (continuous_dist): GaussianNN(\n",
       "      (parameterizer): ParameterizerNN(\n",
       "        (block_dict): ModuleDict(\n",
       "          (input): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=14, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList(\n",
       "              (0-1): 2 x Sequential(\n",
       "                (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "                (1): Softplus(beta=1, threshold=20)\n",
       "              )\n",
       "            )\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "              (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (3): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mu): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList()\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=24, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (sigma): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList()\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=24, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ./runs/Apr23_15-05-37_iepg-st-gpu.ewi.tudelft.nl/trained_model.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = model.log_dir\n",
    "model_name = f'trained_model'\n",
    "model_path = f'./{save_path}/{model_name}.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved at {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioner saved at ./runs/Apr23_15-05-37_iepg-st-gpu.ewi.tudelft.nl/conditioner.pkl\n"
     ]
    }
   ],
   "source": [
    "conditioner_path = f'./{save_path}/conditioner.pkl'\n",
    "with open(conditioner_path, 'wb') as f: pickle.dump(conditioner, f)\n",
    "print(f'Conditioner saved at {conditioner_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
