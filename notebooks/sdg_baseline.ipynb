{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_theme()\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib.vae_models import VAE, CVAE\n",
    "import lib.datasets as datasets\n",
    "import lib.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'goi4_dp_small_2yrs_inpost'\n",
    "RANDOM_SEED = 2112\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1011780 consumption profiles from 730 dates and 1386 users\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'../data/{DATASET_NAME}/dataset.csv')\n",
    "data, dates, users = df.iloc[:,:-2].values, df.date.values, df.user.values\n",
    "date_ids, user_ids = df.date.unique(), df.user.unique()\n",
    "num_days, num_users = len(date_ids), len(user_ids)\n",
    "print(f'Loaded {len(data)} consumption profiles from {num_days} dates and {num_users} users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = np.load(f'../data/{DATASET_NAME}/encode_dict.npy', allow_pickle=True).item()[\"date_dict\"]\n",
    "date_dict_inv = {v: k for k, v in date_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'../data/{DATASET_NAME}/raw_dates.npy'):\n",
    "    raw_dates = np.array([datetime.datetime.strptime(date_dict_inv[d], '%Y-%m-%d') for d in dates])\n",
    "    np.save(f'../data/{DATASET_NAME}/raw_dates.npy', raw_dates)\n",
    "else:\n",
    "    raw_dates = np.load(f'../data/{DATASET_NAME}/raw_dates.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = np.array([d.month for d in raw_dates])\n",
    "weekdays = np.array([d.weekday() for d in raw_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_MONTHS = True\n",
    "ADD_WEEKDAYS = True\n",
    "\n",
    "condition_kwargs[\"tags\"], condition_kwargs[\"types\"], condition_kwargs[\"supports\"]  = [], [], []\n",
    "if ADD_MONTHS: \n",
    "    condition_kwargs[\"tags\"].append(\"months\")\n",
    "    condition_kwargs[\"types\"].append(\"circ\")\n",
    "    condition_kwargs[\"supports\"].append(np.unique(months).tolist())\n",
    "if ADD_WEEKDAYS: \n",
    "    condition_kwargs[\"tags\"].append(\"weekdays\")\n",
    "    condition_kwargs[\"types\"].append(\"circ\")\n",
    "    condition_kwargs[\"supports\"].append(np.unique(weekdays).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioner = datasets.Conditioner(**condition_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_set = conditioner.transform({\"months\": months, \"weekdays\": weekdays})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 1 #in hours\n",
    "\n",
    "if RESOLUTION == 12:\n",
    "    X = np.reshape(data, (-1, 24))\n",
    "    X = np.reshape(np.concatenate([X[:,6:], X[:,:6]], axis=-1), (num_users, num_days, int(24/RESOLUTION), int(RESOLUTION))).sum(axis=-1)    #circle shift the last dimension of X\n",
    "else:\n",
    "    X = np.reshape(data, (num_users, num_days, int(24/RESOLUTION), int(RESOLUTION))).sum(axis=-1)\n",
    "\n",
    "condition_set = np.reshape(condition_set, (num_users, num_days, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 4 users with all-zero consumption profiles\n",
      "Removing 18 users with any-negative consumption profiles\n"
     ]
    }
   ],
   "source": [
    "nonzero_user_mask = np.sum(np.all(X == 0, axis=2), axis=1) < num_days\n",
    "print(f'Removing {(~nonzero_user_mask).sum()} users with all-zero consumption profiles')\n",
    "positive_user_mask = np.sum(np.any(X < 0, axis=2), axis=1) == 0\n",
    "print(f'Removing {(~positive_user_mask).sum()} users with any-negative consumption profiles')\n",
    "user_mask = nonzero_user_mask & positive_user_mask\n",
    "X = X[user_mask]\n",
    "condition_set = condition_set[user_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ampute the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of missing days: 57.19\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "n, a, b = num_days, 0.85, 10.0\n",
    "# n, a, b = num_days, 1.0, 1.0\n",
    "missing_days = np.random.binomial(n, p=np.random.beta(a, b, size=X.shape[0]), size=X.shape[0])\n",
    "print(f\"Mean of missing days: {n*a/(a+b):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing = X.copy().astype(float)\n",
    "conditions_missing = condition_set.copy().astype(float)\n",
    "\n",
    "for user in range(X.shape[0]): \n",
    "    X_missing[user, :missing_days[user]] = np.nan\n",
    "    conditions_missing[user, :missing_days[user]] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsample the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (subsampled/filtered) users....1364\n",
      "Number of (subsampled) days...............730\n",
      "Number of (aggregated) features............24\n"
     ]
    }
   ],
   "source": [
    "USER_SUBSAMPLE_RATE, DAY_SUBSAMPLE_RATE = 1, 1\n",
    "X, X_missing = X[::USER_SUBSAMPLE_RATE, ::DAY_SUBSAMPLE_RATE, :], X_missing[::USER_SUBSAMPLE_RATE, ::DAY_SUBSAMPLE_RATE, :]\n",
    "condition_set, conditions_missing = condition_set[::USER_SUBSAMPLE_RATE, ::DAY_SUBSAMPLE_RATE, :], conditions_missing[::USER_SUBSAMPLE_RATE, ::DAY_SUBSAMPLE_RATE, :]\n",
    "num_users, num_days, num_features = X.shape\n",
    "X_gt_list = [X[user, :missing_days[user]]*1 for user in range(num_users)]\n",
    "X_gt_condition_list = [condition_set[user, :missing_days[user]]*1 for user in range(num_users)]\n",
    "\n",
    "print(\"{:.<40}{:.>5}\".format(\"Number of (subsampled/filtered) users\", num_users))\n",
    "print(\"{:.<40}{:.>5}\".format(\"Number of (subsampled) days\", num_days))\n",
    "print(\"{:.<40}{:.>5}\".format(\"Number of (aggregated) features\", num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_idx_mat  = np.isnan(X_missing).any(2)\n",
    "missing_num_labels = {\"user\": missing_idx_mat.sum(1), \"day\": missing_idx_mat.sum(0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing = X_missing.reshape(-1, num_features)\n",
    "conditions_missing = conditions_missing.reshape(-1, conditioner.cond_dim)\n",
    "missing_idx = np.isnan(X_missing.sum(1))\n",
    "X_missing = X_missing[~missing_idx]\n",
    "conditions_missing = conditions_missing[~missing_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Training Data with Missing Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_mean, nonzero_std = utils.zero_preserved_log_stats(X_missing)\n",
    "X_missing = utils.zero_preserved_log_normalize(X_missing, nonzero_mean, nonzero_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the dataset using skitlearn\n",
    "VAL_RATIO = 0.01\n",
    "\n",
    "X_train, X_val, conditions_train, conditions_val = train_test_split(X_missing, conditions_missing, test_size=0.1, random_state=RANDOM_SEED)\n",
    "X_train, X_val, conditions_train, conditions_val = torch.tensor(X_train).float(), torch.tensor(X_val).float(), torch.tensor(conditions_train).float(), torch.tensor(conditions_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Points: 825447\n",
      "Number of Validation Points: 91717\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.ConditionedDataset(inputs=X_train, conditions=conditions_train)\n",
    "valset = datasets.ConditionedDataset(inputs=X_val, conditions=conditions_val)\n",
    "print(f\"Number of Training Points: {len(trainset)}\")\n",
    "print(f\"Number of Validation Points: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "                \"latent_dim\": 10,\n",
    "                \"condition_dim\": conditioner.cond_dim,\n",
    "                \"condencoding_dim\": None,\n",
    "                \"posterior_dist\": 'normal',\n",
    "                \"likelihood_dist\": 'mixed',\n",
    "                \"learn_decoder_sigma\": True,\n",
    "                \"num_neurons\": 100,\n",
    "                \"num_hidden_layers\": 3,\n",
    "                \"dropout\": True,\n",
    "                \"dropout_rate\": 0.25,\n",
    "                \"batch_normalization\": True,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(input_dim=num_features, **model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = {\n",
    "                \"lr\": 1e-4,\n",
    "                \"beta\": 1.0,\n",
    "                \"num_mc_samples\": 1,\n",
    "                \"epochs\": 1000,\n",
    "                \"verbose_freq\": 100,\n",
    "                \"tensorboard\": True,\n",
    "                \"batch_size\": 1024,\n",
    "                \"validation_freq\": 100\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_kwargs[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=train_kwargs[\"batch_size\"], drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]/home/kbolat/miniconda3/envs/torchenv/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Epoch:   0%|          | 1/1000 [00:01<22:12,  1.33s/it]/home/kbolat/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "                                                       \n",
      "Epoch:   0%|          | 1/1000 [00:26<22:12,  1.33s/it]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- ELBO=-5.94e+01 / RLL=-5.67e+01 / KL=2.64e+00\n",
      "Iteration: 100 -- ELBO=-6.28e+01 / RLL=-5.99e+01 / KL=2.83e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/behavio-temporal-vae/lib/vae_models.py:152\u001b[0m, in \u001b[0;36mVAE.fit\u001b[0;34m(self, trainloader, valloader, lr, beta, num_mc_samples, epochs, verbose_freq, tensorboard, tqdm_func, validation_freq, **_)\u001b[0m\n\u001b[1;32m    150\u001b[0m itx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    151\u001b[0m pbar_itx\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m## region Validation\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m itx\u001b[38;5;241m%\u001b[39mvalidation_freq\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m itx\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m valloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Python/behavio-temporal-vae/lib/vae_models.py:231\u001b[0m, in \u001b[0;36mCVAE.train_core\u001b[0;34m(self, inputs, optim, **_)\u001b[0m\n\u001b[1;32m    229\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    230\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(inputs, x_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m], z_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m], beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m], prior_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_params)\n\u001b[0;32m--> 231\u001b[0m \u001b[43mloss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(trainloader=trainloader, valloader=valloader, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CVAE(\n",
       "  (encoder): GaussianNN(\n",
       "    (parameterizer): ParameterizerNN(\n",
       "      (block_dict): ModuleDict(\n",
       "        (input): NNBlock(\n",
       "          (input_layer): Sequential(\n",
       "            (0): Linear(in_features=28, out_features=50, bias=True)\n",
       "            (1): Softplus(beta=1, threshold=20)\n",
       "          )\n",
       "          (middle_layers): ModuleList(\n",
       "            (0-1): 2 x Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "          )\n",
       "          (output_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (1): Softplus(beta=1, threshold=20)\n",
       "            (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mu): NNBlock(\n",
       "          (input_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (1): Softplus(beta=1, threshold=20)\n",
       "          )\n",
       "          (middle_layers): ModuleList()\n",
       "          (output_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (sigma): NNBlock(\n",
       "          (input_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "            (1): Softplus(beta=1, threshold=20)\n",
       "          )\n",
       "          (middle_layers): ModuleList()\n",
       "          (output_layer): Sequential(\n",
       "            (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): MixedNN(\n",
       "    (discerete_dist): BernoulliNN(\n",
       "      (parameterizer): ParameterizerNN(\n",
       "        (block_dict): ModuleDict(\n",
       "          (input): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=14, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList(\n",
       "              (0-1): 2 x Sequential(\n",
       "                (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "                (1): Softplus(beta=1, threshold=20)\n",
       "              )\n",
       "            )\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "          )\n",
       "          (pi): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList()\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=24, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (continuous_dist): GaussianNN(\n",
       "      (parameterizer): ParameterizerNN(\n",
       "        (block_dict): ModuleDict(\n",
       "          (input): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=14, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList(\n",
       "              (0-1): 2 x Sequential(\n",
       "                (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "                (1): Softplus(beta=1, threshold=20)\n",
       "              )\n",
       "            )\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "              (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (3): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mu): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList()\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=24, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (sigma): NNBlock(\n",
       "            (input_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "              (1): Softplus(beta=1, threshold=20)\n",
       "            )\n",
       "            (middle_layers): ModuleList()\n",
       "            (output_layer): Sequential(\n",
       "              (0): Linear(in_features=50, out_features=24, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (condencoder): Identity()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ./runs/Apr22_16-52-52_iepg-st-gpu.ewi.tudelft.nl/trained_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "save_path = model.log_dir\n",
    "model_name = f'trained_model'\n",
    "model_path = f'./{save_path}/{model_name}.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved at {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioner saved at ./runs/Apr22_16-52-52_iepg-st-gpu.ewi.tudelft.nl/conditioner.pkl\n"
     ]
    }
   ],
   "source": [
    "conditioner_path = f'./{save_path}/conditioner.pkl'\n",
    "with open(conditioner_path, 'wb') as f: pickle.dump(conditioner, f)\n",
    "print(f'Conditioner saved at {conditioner_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
